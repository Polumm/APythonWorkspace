##  一、爬虫分类

### 通用爬虫

​		抓取系统重要组成部分，一般对整张页面进行抓取。比如：利用通用爬虫抓取导师简介页面所有老师的姓名缩写和个人URL地址

### 聚焦爬虫

​		建立在通用爬虫基础上，抓取特定页面的特定的局部内容。比如：在初次爬取后，某每个老师个人页面的个人基本信息部分进行爬取。

### 增量式爬虫

​		检测网站中最新更新出来的数据



## 二、超文本传输协议

### http协议

#### 请求方式

> ​		根据 HTTP 标准，HTTP 请求可以使用多种请求方法。
>
> ​		HTTP1.0 定义了三种请求方法： GET, POST 和 HEAD 方法。
>
> ​		HTTP1.1 新增了五种请求方法：OPTIONS, PUT, DELETE, TRACE 和 CONNECT 方法。
>
> 

| 请求方式 | 描述                                                         |
| -------- | :----------------------------------------------------------- |
| GET      | 请求指定的页面信息，并返回实体主体。                         |
| HEAD     | 类似于 get 请求，只不过返回的响应中没有具体的内容，用于获取报头 |
| POST     | 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。 |
| PUT      | 从客户端向服务器传送的数据取代指定的文档的内容               |
| DELETE   | 请求服务器删除指定的页面。                                   |
| CONNECT  | HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。    |
| OPTIONS  | 允许客户端查看服务器的性能。                                 |
| TRACE    | 回显服务器收到的请求，主要用于测试或诊断。                   |
|          |                                                              |
|          |                                                              |

​		简单来说：get向网页请求数据，post上传数据



#### 常用请求头信息

​		- User-Agent：==请求载体的身份标识==

​				包含了用户浏览器的请求方式、主机名、浏览器类型及版本、Cookie

​		1、为什么要设置headers?

​			在请求网页爬取的时候，输出的text信息中会出现抱歉，无法访问等字眼，这就是禁止爬取，**需要反爬机制**去解决这个问题。

​			headers是解决requests请求反爬的方法之一，相当于我们进去这个网页的服务器本身，**假装是用户的正常访问行为**。

​			对反爬虫网页，可以设置一些headers信息，模拟成浏览器取访问网站 。

​		2、 headers在哪里找？

​			谷歌或者火狐浏览器，在网页面上点击：右键–>检查–>剩余按照图中显示操作，需要按Fn+F5刷新出网页来

​		- Connection：请求成功后，是断开连接还是保持连接 close/keep alive

#### 常用响应头信息

​		- Content-Type：服务器响应回客户端的数据类型

### https

​		- **安全的**超文本传输协议（数据加密）

#### 加密方式

  - 对称密钥加密
  - 非对称密钥加密
  - 证书密钥加密



## 三、requests模块与请求流程分析

不要再犯这种憨憨错误，你文件名不要和库名冲突了好吧

```
D:\Program\Anaconda\python.exe D:/APythonWorkSpace/ReLifeProject/爬虫.py
Traceback (most recent call last):
  File "D:/APythonWorkSpace/ReLifeProject/爬虫.py", line 7, in <module>
    import requests as rq
  File "D:\APythonWorkSpace\ReLifeProject\requests.py", line 5, in <module>
    response = requests.get(test_url)
AttributeError: partially initialized module 'requests' has no attribute 'get' (most likely due to a circular import)

进程已结束，退出代码为 1

```

### 作用：

​		模拟浏览器发请求

### 如何使用：

  - 指定URL
  - 基于requests模块发起请求
  - 获取响应对象的数据值
  - 存储 

### requests.get()

```python
# 导入模块
import requests
# 定义请求地址
url = 'http://grzy.cug.edu.cn/'
# 发送 GET, 请求获取响应
response = requests.get(url)
# 获取响应的 html 内容
html = response.text
print(html)
```

利用 requests.get(url) 获取了页面信息主体，我们将对象实体其命名为response

 - response.text 返回响应内容，响应内容为 str 类型
 - respones.content 返回响应内容,响应内容为 bytes 类型
- response.status_code 返回响应状态码
- response.request.headers 返回请求头
- response.headers 返回响应头
- response.cookies 返回响应的 RequestsCookieJar 对象

有个疑问，为什么有response.text 还要先response.content 再转str类呢？

​		我猜可能是为了明确规定采用的编码方案：

```python
html = response.content.decode('utf-8')
print(html)
```

### 自定义请求头

```python
# 导入模块
import requests
# 定义请求地址
url = 'http://grzy.cug.edu.cn/'
# 定义自定义请求头
headers = {
  "User-Agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36"
}
# url + 请求头 发送get请求
response = requests.get(url,headers=headers)
# 获取响应的 html 内容
html = response.content.decode('utf-8')
print(html)
```



### 待续内容

#### 带params参数的GET请求

#### 发送Post请求

#### 发送携带 Cookies的请求

#### 各种异常处理

#### 保存图片

```python
# 导入模块
import requests
# 下载图片地址
url = "http://docs.python-requests.org/zh_CN/latest/_static/requests-sidebar.png"
# 发送请求获取响应
response = requests.get(url)
# 保存图片
with open('image.png','wb') as f:
  f.write(response.content)
```

​		保存图片时后缀名和请求的后缀名一致

​		保存必须使用 response.content 进行保存文件



## 四、数据提取

### 什么是数据提取

> 简单的来说，数据提取就是从响应中获取我们想要的数据的过程
>
> ### 数据的种类
>
> 数据可分为**结构化数据**和**非结构化数据**
>
> #### 结构化数据
>
> 处理方式：**通过 json 模块等直接转成 Python 数据类型**
>
> json格式
>
> ```json
> {
>   "name":"hello",
>   "age":18,
>   "parents":{
>     "mother":"妈妈",
>     "father":"爸爸"
>   }
> }
> ```
>
> xml格式
>
> ```xml
> <bookstore>
>   <book category="COOKING">
>     <title lang="en">Everyday Italian</title> 
>     <author>Giada De Laurentiis</author> 
>     <year>2005</year> 
>     <price>30.00</price> 
>   </book>
>   <book category="CHILDREN">
>     <title lang="en">Harry Potter</title> 
>     <author>J K. Rowling</author> 
>     <year>2005</year> 
>     <price>29.99</price> 
>   </book>
>   <book category="WEB">
>     <title lang="en">Learning XML</title> 
>     <author>Erik T. Ray</author> 
>     <year>2003</year> 
>     <price>39.95</price> 
>   </book>
> </bookstore>
> ```
>
> #### 非结构化数据
>
> 处理方式：==通过 `正则表达式` 、 `xpath` 、`beautifulsoup` 等模块提取数据==
>
> 数据类型
>
> - html 格式数据
> - word 格式数据等



## 五、JSON 数据提取

施工中~

## 六、re模块实现非结构化数据的正则提取

利用Python中的re模块和正则表达式进行数据的提取

[正则表达式光速入门]([正则表达式30分钟入门教程 (deerchao.cn)](https://deerchao.cn/tutorials/regex/regex.htm))

## 1. re模块的使用过程

在Python中需要通过正则表达式对字符串进行匹配的时候，可以使用一个模块，名字为re

```
    # 导入re模块
    import re

    # 使用match方法进行匹配操作
    result = re.match(正则表达式,要匹配的字符串)

    # 如果上一步匹配到数据的话，可以使用group方法来提取数据
    result.group()
```







举个栗子：

### 【知识储备】

#### **贪婪 vs. 非贪婪匹配**

以"hello school html hlight hl"为例

 **(.\*)** 是**贪婪匹配**代表==尽可能多的匹配字符==因此它将h和l之间所有的字符都匹配了出来

言外之意：只匹配最长的**一次匹配**

![](https://img2018.cnblogs.com/blog/1847035/201910/1847035-20191027191934326-1741657583.png)



**(.\*?)** 是**非贪婪匹配**（懒惰匹配）尽可能匹配少的字符

但是==**要匹配出所有的字符**==（在**通用爬虫的匹配**过程中，同类**重复匹配**就需要）

![](https://img2018.cnblogs.com/blog/1847035/201910/1847035-20191027192211036-1802386056.png)

为了实现同类重复匹配，我们用到了**(\.\*?)**,但是如果要指定中间字符匹配的长度（不是最短）

可以用：**.{n,m}?**代表匹配数量从n到m数量的字符

![](https://img2018.cnblogs.com/blog/1847035/201910/1847035-20191027192722102-2078561491.png) 

![](https://img2018.cnblogs.com/blog/1847035/201910/1847035-20191027192756275-1912128522.png)

==**\(\.\*\)是贪婪的，所以它只匹配全文中最长的那一个，如果每个都要匹配就必须\(\.\*?\)**==



#### **分组捕捉**

| 分类     | 代码/语法    | 说明                                                         |
| -------- | ------------ | ------------------------------------------------------------ |
| 捕获     | (exp)        | 匹配exp,并捕获文本到自动命名的组里                           |
|          | (?<name>exp) | 匹配exp,并捕获文本到名称为name的组里，也可以写成(?'name'exp) |
|          | (?:exp)      | 匹配exp,不捕获匹配的文本，也不给此分组分配组号               |
| 零宽断言 | (?=exp)      | 匹配exp前面的位置                                            |
|          | (?<=exp)     | 匹配exp后面的位置                                            |
|          | (?!exp)      | 匹配后面跟的不是exp的位置                                    |
|          | (?<!exp)     | 匹配前面不是exp的位置                                        |
| 注释     | (?#comment)  | 这种类型的分组不对正则表达式的处理产生任何影响，用于提供注释让人阅读 |

实现通用爬虫的功能，要分组捕捉每个老师个人页面的url和姓名，非贪婪**多次捕捉**(\.\*?)，当然在这前后限定好捕捉的位置



##### re.compile实现重复利用正则对象，高效匹配

> re.compile()是用来优化正则的，它==**将正则表达式转化为对象**==，re.search(pattern, string)的调用方式就转换为 pattern.search(string)的调用方式，多次调用一个正则表达式就**重复利用这个正则对象**，可以实现更有效率的匹配

注意着：**re.compile()生成的是正则对象，==单独使用没有任何意义==，需要和findall(), search(), match(）搭配使用**

##### **常用方法findall & search & match**

> findall 
>            findall('正则','待匹配的字符串')
> 　　　　　　依据正则查找字符串中**所有符合该正则的匹配内容**,然后组织成一个列表的形式返回
> search
>            search('正则','待匹配的字符串')
> 　　　　　　依据正则查找字符串中**第一个**符合该正则的匹配内容,然后**立即结束本次查找** 返回一个结果对象
>            当匹配到的值得时候 查看对象里面的值 用 对象.group()
>            当匹配不到值得时候 返回的是一个None 对象.group()就直接报错 因为None没有内置的group()方法
> match
>            match('正则','待匹配的字符串')
> 　　　　　　依据正则查找**字符串开头**是否符合该正则的匹配内容,如果有然后立即结束本次查找 返回一个结果对象
> 　　　　　　当匹配到的值得时候 查看对象里面的值 用 对象.group()
> 　　　　　　当匹配不到值得时候 返回的是一个None 对象.group()就直接报错 因为None没有内置的group()方法
> ps:
>    **通常用search和match，为防止报错，用if判断**
>     **res = search/match('正则','待匹配的字符串')**
>     **if res:**
>       **print(res.group())**

 - **match** 开头匹配，只匹配一次

- **search** 全局匹配，只匹配一次(只匹配第一个)

- **findall** 匹配所有符号条件的数据，返回是 结果列表
- **finditer** 迭代对象，迭代 Match 对象

##### 分组与替换方法

###### 分组

> 通过给定字符串进行对数据进行分组

```
#!/usr/bin/python3
# -*- coding: utf-8 -*-

import re
string = "a;dj jkl,jj; j;sd"
# split 分组
pattern = re.compile(r'[; ,]+')
result = pattern.split(string)
print(result)
```

###### 替换

> 通过给定的正则表达式和替换字符进行替换

```
#!/usr/bin/python3
# -*- coding: utf-8 -*-

import re

# sub 交换
string = "hello world;sjd;ssdjkls;sdjk;crise lyj"
# 带 空格的词组替换成 #
pattern = re.compile(r'(\w+) (\w+)')

# 把 空格的词组 进行交换
result = pattern.sub(r"\2 \1",string)

print(result)
```

##### 关于compile的一些个参数

正则表达式可以包含一些可选标志修饰符来控制匹配的模式。修饰符被指定为一个可选的标志。多个标志可以通过按位 OR(|) 它们来指定。如 re.I | re.M 被设置成 I 和 M 标志：

**==可以作为compile的参数，修改一些元字符的匹配条件==**

| 修饰符 | 描述                                                         |
| ------ | ------------------------------------------------------------ |
| re.I   | 使匹配对大小写不敏感                                         |
| re.L   | 做本地化识别（locale-aware）匹配                             |
| re.M   | 多行匹配，影响 ^ 和 $                                        |
| re.S   | 使 . 匹配包括换行在内的所有字符                              |
| re.U   | 根据Unicode字符集解析字符。这个标志影响 \w, \W, \b, \B.      |
| re.X   | 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解。 |



### **举一个栗子看看常见的提取流程：**

需求：从导师简介总页面中，提取出每个老师个人页面的url地址，以及教师姓名

![image-20220122204035892](https://cdn.jsdelivr.net/gh/Polumm/PicGoo/image-20220122204035892.png) 



 1. **拷贝原有字符串内容		*get()再\.content\.decode\('utf-8'\)***
 2. **把所要提取的数据使用 `(.*)` 或 `(.*?)` 进行分组/替换       *compile\(表达式, 参数\) 生成正则表达式对象，中间\(\.\*?\)实现批量分组捕捉***
 3. **使用 `findall` 或 `finditer` 进行数据提取      findall(正则对象, 文本)**

```python
 response = rq.get(url, headers=headers, verify=False)
 text = response.content.decode('utf-8')
 pattern = re.compile('<div class="media-caption">.*?href="(.*?)">(.*?)</a></h2>', re.S)
 results = re.findall(pattern, text)
```







