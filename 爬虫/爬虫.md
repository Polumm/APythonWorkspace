## 一、爬虫分类

### 通用爬虫

​		抓取系统重要组成部分，一般对整张页面进行抓取。比如：利用通用爬虫抓取导师简介页面所有老师的姓名缩写和个人URL地址

### 聚焦爬虫

​		建立在通用爬虫基础上，抓取特定页面的特定的局部内容。比如：在初次爬取后，某每个老师个人页面的个人基本信息部分进行爬取。

### 增量式爬虫

​		检测网站中最新更新出来的数据



## 二、超文本传输协议

### http协议

#### 请求方式

> ​		根据 HTTP 标准，HTTP 请求可以使用多种请求方法。
>
> ​		HTTP1.0 定义了三种请求方法： GET, POST 和 HEAD 方法。
>
> ​		HTTP1.1 新增了五种请求方法：OPTIONS, PUT, DELETE, TRACE 和 CONNECT 方法。
>
> 

| 请求方式 | 描述                                                         |
| -------- | :----------------------------------------------------------- |
| GET      | 请求指定的页面信息，并返回实体主体。                         |
| HEAD     | 类似于 get 请求，只不过返回的响应中没有具体的内容，用于获取报头 |
| POST     | 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。 |
| PUT      | 从客户端向服务器传送的数据取代指定的文档的内容               |
| DELETE   | 请求服务器删除指定的页面。                                   |
| CONNECT  | HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。    |
| OPTIONS  | 允许客户端查看服务器的性能。                                 |
| TRACE    | 回显服务器收到的请求，主要用于测试或诊断。                   |
|          |                                                              |
|          |                                                              |

​		简单来说：get向网页请求数据，post上传数据



#### 常用请求头信息

​		- User-Agent：==请求载体的身份标识==

​				包含了用户浏览器的请求方式、主机名、浏览器类型及版本、Cookie

​		1、为什么要设置headers?

​			在请求网页爬取的时候，输出的text信息中会出现抱歉，无法访问等字眼，这就是禁止爬取，**需要反爬机制**去解决这个问题。

​			headers是解决requests请求反爬的方法之一，相当于我们进去这个网页的服务器本身，**假装是用户的正常访问行为**。

​			对反爬虫网页，可以设置一些headers信息，模拟成浏览器取访问网站 。

​		2、 headers在哪里找？

​			谷歌或者火狐浏览器，在网页面上点击：右键–>检查–>剩余按照图中显示操作，需要按Fn+F5刷新出网页来

​		- Connection：请求成功后，是断开连接还是保持连接 close/keep alive

#### 常用响应头信息

​		- Content-Type：服务器响应回客户端的数据类型

### https

​		- **安全的**超文本传输协议（数据加密）

#### 加密方式

  - 对称密钥加密
  - 非对称密钥加密
  - 证书密钥加密



## 三、requests模块与请求流程分析

不要再犯这种憨憨错误，你文件名不要和库名冲突了好吧

```
D:\Program\Anaconda\python.exe D:/APythonWorkSpace/ReLifeProject/爬虫.py
Traceback (most recent call last):
  File "D:/APythonWorkSpace/ReLifeProject/爬虫.py", line 7, in <module>
    import requests as rq
  File "D:\APythonWorkSpace\ReLifeProject\requests.py", line 5, in <module>
    response = requests.get(test_url)
AttributeError: partially initialized module 'requests' has no attribute 'get' (most likely due to a circular import)

进程已结束，退出代码为 1

```

### 作用：

​		模拟浏览器发请求

### 如何使用：

  - 指定URL
  - 基于requests模块发起请求
  - 获取响应对象的数据值
  - 存储 

### requests.get()

```python
# 导入模块
import requests
# 定义请求地址
url = 'http://grzy.cug.edu.cn/'
# 发送 GET, 请求获取响应
response = requests.get(url)
# 获取响应的 html 内容
html = response.text
print(html)
```

利用 requests.get(url) 获取了页面信息主体，我们将对象实体其命名为response

 - response.text 返回响应内容，响应内容为 str 类型
 - respones.content 返回响应内容,响应内容为 bytes 类型
- response.status_code 返回响应状态码
- response.request.headers 返回请求头
- response.headers 返回响应头
- response.cookies 返回响应的 RequestsCookieJar 对象

有个疑问，为什么有response.text 还要先response.content 再转str类呢？

​		我猜可能是为了明确规定采用的编码方案：

```python
html = response.content.decode('utf-8')
print(html)
```

### 自定义请求头

```python
# 导入模块
import requests
# 定义请求地址
url = 'http://grzy.cug.edu.cn/'
# 定义自定义请求头
headers = {
  "User-Agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36"
}
# url + 请求头 发送get请求
response = requests.get(url,headers=headers)
# 获取响应的 html 内容
html = response.content.decode('utf-8')
print(html)
```



### 待续内容

#### 带params参数的GET请求

#### 发送Post请求

#### 发送携带 Cookies的请求

#### 各种异常处理

#### 保存图片

```python
# 导入模块
import requests
# 下载图片地址
url = "http://docs.python-requests.org/zh_CN/latest/_static/requests-sidebar.png"
# 发送请求获取响应
response = requests.get(url)
# 保存图片
with open('image.png','wb') as f:
  f.write(response.content)
```

​		保存图片时后缀名和请求的后缀名一致

​		保存必须使用 response.content 进行保存文件



## 四、数据提取
